---
title: "Comparison of temporal indices"
bibliography: references.bib
output: 
  tufte::tufte_html: 
    tufte_variant: "envisioned"
    tufte_features: ["italics"]
    css: "style.css"
link-citations: yes
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")))
```

# TODO

- [ ] Add parallelism
- [x] Implement adaptive period index
- [x] Count the intervals touched by each query, to have an
      implementation-independent metric to compare the elements.
      Then as a performance metric we can consider the number of intervals examined normalized by the output size,
      with good algorithms having a ratio close to 1.

## Questions

The three points below have been addressed by PeriodIndex++.

- [x] Why don't we make the period index adaptive also on the duration? That way we 
      can lift the assumption that durations are zipf distributed
- [x] Can we do without the replication of intervals?
- [x] There seems to be a tradeoff with the number of buckets: more means fewer intervals analyzed but also more duplication.

## Observations

In the last couple of years there has been a huge volume of work on "learned
indices". It remains unclear the scope in which these outperform traditional
indices. In particular, a very recent paper on Arxiv [@Marcus2020BenchmarkingLI], which has a co-author one
of the original proponents of learned indices, claims that such structures work
well in "read-only in-memory workloads over a dense array". To me, this seems
like a pretty narrow scope. 

- [ ] The question is thus: can we make a learned index
that works well in the more general case? That that can be updated efficiently?

# Research questions

1. What is the best algorithm in terms of throughput, for different query workloads?
1. How does the throughput change with the size of the dataset?
1. How robust are the algorithms to changes in the query workload? I.e., is the query time stable?
1. How are query times distributed? 

# The algorithms involved

All the data structures are implemented in memory, using Rust `1.44.0-nightly`.
Compared to the Java implementation of the conference version, this has the
advantage of allowing manual memory management, thus avoiding all the issues that
Java garbage collector brings to the table when measuring performance.

Time chronons are 32 bit integers, and intervals are pairs of chronons, with the end
being excluded from the interval.

`LinearScan`

:   Builds no index, just scans the entire dataset for each query. This sets a baseline,
    and should be the slowest performing algorithm.
    The implementation stores all the intervals in a in-memory vector.

`IntervalTree`

:   A classic data structure for range queries, which does not support duration queries.
    the implementation is based on the linked data structure described in [@Kriegel2000ManagingIE].
    Is expected to perform well on range queries, while for duration queries it boils down to a linear scan of the input.

`BTree`

:   Possibly the most classical of all classical data structures in databases.
    In this context it is used to index _durations_: all the intervals with the same
    duration are associated to the same node of the tree.
    The implementation uses Rust's highly optimized [BTreeMap](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html), which is faster than anything I might
    roll down manually.
    Note that this data structure is also in-memory. Rather than using it to optimize
    block transfers from the disk to the main memory (as in classic database systems),
    Rust's developers use the BTree to optimize the transfers between the
    main memory and the _processor cache_.

    This data structure performs well on the _duration_ part of the query, while for
    pure range queries it is equivalent to a linear scan of the dataset.

    Note that this data structure is *not* a `B+-tree`, so iteration over ranges is not
    super efficient because there are no links between adjacent leaves.

`Grid2D`

:   An adaptive two-dimensional grid, which indexes intervals as if they were
    points in a two-dimensional space.
    To query this data structure, we find all the grid cells that can overlap
    with the query, like in the shaded area of the following figure

    ```
      end times     +.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
      +------------------------------+ Query, with
                    |                   swapped endpoints.
                    |
                    |
                    +---------------------------------------+
                                                        start times
    ```

    The data structure is adaptive in the sense that rather than fixing the 
    breakpoints between grid cells at fixed intervals, we use the empirical cumulative
    distribution functions of both the start and end boundaries of the intervals to
    put the same amount of points in all the cells.

    This data structure is expected to behave well on range queries, with
    duration queries being equivalent to linear scans.

    The data structure is parameterized by the number of cells along each dimension.

`Grid3D`

:   Just like its two-dimensional counterpart, but adds a third dimension for
    the duration, also using an adaptive approach. Is expected to work well for
    range-duration queries.

    I couldn't find references for this in the literature. Therefore, while simple, 
    it might be novel.

    The data structure is parameterized by the number of cells along each dimension.

`PeriodIndex`

:   The data structure proposed in the paper. This is the static variant, I have
    yet to tackle the adaptive one.
    Should perform well on both range and duration queries.

    My concern with this data structure is that each interval is potentially stored multiple times. This is a unicum among the considered data structures, and requires
    removing the duplicates. While this operation can be done in constant time, it
    does not come for free.

    The data structure is parameterized by the number of buckets and the maximum number of levels within each bucket.

`NestedBTree` and `NestedVecs`

:   Both data structures are based on the same idea: intervals are partitioned by duration, with indexing in
    place to be able to access the group of intervals with a given duration efficiently. Within each group,
    intervals are indexed by their start time.^[End times can be omitted, since intervals in the same group
    all share the same length.]
    Both indices should allow for efficient range queries.

    Querying works as follows. For a given range of durations to be queried, we select all the groups whose 
    duration falls within the query range. Then, let $(s, e)$ be the time range use for the overlap part
    of the query. On group associated with the duration $d$, we get all the start points $x$ such that
    \[
      s - d < x < e
    \]

    The two implementations use different means of answering the range queries. `NestedBTree` uses a `BTree`
    to map duration to each subgroup. Each subgroup is then indexed with another `BTree on the starting`
    times.
    `NestedVecs` instead keeps all the starting times of intervals of the same duration in a sorted array.
    These arrays are then arranged themselves in an array sorted by the duration of the group.
    Queries are then answered by means of binary search.
    This second implementation has the advantage of begin very cache efficient in the iteration within the
    returned range, but has the drawback of not being implementable in an actual database.
    We use it to see how fast this approach can potentially go.

## Dataset distribution

The following plots report the distribution of times in the dataset we use in the following
experiments, which comprises $10\,000\,000$ intervals with start times uniformly
distributed between 0 and 10 million, and durations distributed according to a Zipf
law with $\beta=1$.

```{r fig.fullwidth=TRUE}
drake::readd(plot_distribution)
```

The plots confirm that we are getting the distributions we ask for.
Note that to allow distributions to have the distributions we want,
we have to allow duplicate intervals.
Otherwise, especially for the durations, it is almost impossible to have a lot of 
small values.

## An overview of the results

```{r fig.fullwidth=F}
drake::readd(overview_qps)
```

## Range-duration queries

We consider a dataset of $n=10\,000\,000$ intervals, with starting times uniformly distributed between
0 and $n$.
Durations are distributed according to a zipf law with exponent 1.
^[The experiments have beed run on the host `ironmaiden`]

The aim of this experiment is to test a workload where the overlap of intervals
plays some role, unlike the workload used in the conference version of the paper (see the appendix).

The following figure reports the throughput of each algorithm. Parameterized algorithms report
several parameterizations.

```{r fig.fullwidth=TRUE, fig.width=10, fig.cap="Query throughput (in queries per second) of different indices with different parameterization, on a dataset of ten million intervals."}
readd(plot_both)
```

The `NestedVecs` data structure is by far the best performing, followed by the three dimensional grid.
`NestedBTree` is quite a lot slower than its vector-based sibiling: using a linked structure apparently
leaves quite a lot of performance on the table.
All the other algorithms are in a completely different league, performing pretty similarly to one another
(at least in terms of order of magnitude) and being one to two orders of magnitude slower than the leading
group.

In the next figure we consider the distribution of query times of the 5000 queries (left), for the best
configuration of each algorithm. The algorithms are sorted by increasing median query time, which is reported
below the little triangle. The number shown beside the small arrow is the $90$-th percentile query time.

To take into account the fact that a query with many results requires some time just to iterate through the 
output, we also consider the _normalized query time_, which is an _output sensitive_ measure: we divide
the query time by the size of the output of the given query (right plot).

```{r fig.fullwidth=TRUE, fig.width=10, fig.height=6, fig.dpi=300}
#plot_grid(
  #readd(plot_latency_both),
  #readd(plot_normalized_latency_both)
#)
```

Looking at both metrics the ranking between the algorithms does not really change.
The interesting thing is that the ranking changes between this pair of figures and the previous
one.
Now `PeriodIndex` ranks second, with a median query time pretty close to the very fast
`NestedVecs` data structure, whereas in terms of throughput it falls very far behind.
This difference is explained by looking at the 90-th percentile of the query time, which shows how 
`PeriodIndex` has and _extremely_ long tail: $10\%$ of the intervals have a query time that is
two orders of magnitude slower than the median!

```{r fig.fullwidth=TRUE, fig.widht=10, fig.height=3, fig.dpi=300, fig.caption="Ratio between number of intervals touched during a query and the output size of the query. Lower (i.e. 1) is better."}
#readd(plot_overhead_both)
```

## Range only queries 

This is the same workload as before, with the duration part of the query omitted.
My expectation here is for data structures that explicitly support range queries (all except linear scan and `BTree`) to perform well.

```{r fig.fullwidth=TRUE, fig.widht=10}
readd(plot_range_only)
```

The plot above meets the expectations, but the throughput is lower.
The reason is explained by distribution plots below: 

```{r fig.fullwidth=TRUE, fig.width=10, fig.height=6, fig.dpi=300}
#plot_grid(
  #readd(plot_latency_range),
  #readd(plot_normalized_latency_range)
#)
```

Looking at the normalized query time we see that it is actually slower than with the range-duration 
workload: the reduced throughput is thus just a consequence of the larger size of the output.

We also observe a phenomenon similar to the one observed in the previous section: the interval tree is
the fastest to answer queries but its distribution has a very long tail, which hurts the throughput.

What surprises me is the performance of `NestedVecs`, which I expected to be better. Apparently 
doing several binary searches is detrimental for performance.

```{r fig.fullwidth=TRUE, fig.widht=10, fig.height=3, fig.dpi=300, fig.caption="Ratio between number of intervals touched during a query and the output size of the query. Lower (i.e. 1) is better."}
#readd(plot_overhead_both)
```

## Duration only queries 

Again, we have a similar workload here, but just with the duration part. The expectation now is
to have `BTree` to perform really well.

```{r fig.fullwidth=TRUE, fig.widht=10}
readd(plot_duration_only)
```

The performance of `NestedVecs` is pretty similar.
Curiously, the period index does not support well this workload.

As for the distribution of query times we have that the top ranked algorithms have a pretty
concentrated distribution.

```{r fig.fullwidth=TRUE, fig.width=10, fig.height=6, fig.dpi=300}
#plot_grid(
  #readd(plot_latency_duration),
  #readd(plot_normalized_latency_duration)
#)
```

```{r fig.fullwidth=TRUE, fig.widht=10, fig.height=3, fig.dpi=300, fig.caption="Ratio between number of intervals touched during a query and the output size of the query. Lower (i.e. 1) is better."}
#readd(plot_overhead_duration)
```
