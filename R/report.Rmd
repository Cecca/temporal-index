---
title: "Comparison of temporal indices"
bibliography: references.bib
output: 
  tufte::tufte_html: 
    tufte_variant: "envisioned"
    tufte_features: ["italics"]
    css: "style.css"
link-citations: yes
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```

# TODO

- [ ] Add parallelism
- [ ] Implement adaptive period index
- [ ] Count the intervals touched by each query, to have an implementation-independent metric to compare the elements.
      Then as a performance metric we can consider the number of intervals examined normalized by the output size,
      with good algorithms having a ratio close to 1.

## Questions

- [ ] Why don't we make the period index adaptive also on the duration? That way we 
      can lift the assumption that durations are zipf distributed
- [ ] Can we do without the replication of intervals?

# Research questions

1. What is the best algorithm in terms of throughput, for different query workloads?
1. How does the throughput change with the size of the dataset?
1. How robust are the algorithms to changes in the query workload? I.e., is the query time stable?
1. How are query times distributed? 

# The algorithms involved

All the data structures are implemented in memory, using Rust `1.44.0-nightly`.
Compared to the Java implementation of the conference version, this has the
advantage of allowing manual memory management, thus avoiding all the issues that
Java garbage collector brings to the table when measuring performance.

Time chronons are 32 bit integers, and intervals are pairs of chronons, with the end
being excluded from the interval.

`LinearScan`

:   Builds no index, just scans the entire dataset for each query. This sets a baseline,
    and should be the slowest performing algorithm.
    The implementation stores all the intervals in a in-memory vector.

`IntervalTree`

:   A classic data structure for range queries, which does not support duration queries.
    the implementation is based on the linked data structure described in [@Kriegel2000ManagingIE].
    Is expected to perform well on range queries, while for duration queries it boils down to a linear scan of the input.

`BTree`

:   Possibly the most classical of all classical data structures in databases.
    In this context it is used to index _durations_: all the intervals with the same
    duration are associated to the same node of the tree.
    The implementation uses Rust's highly optimized [BTreeMap](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html), which is faster than anything I might
    roll down manually.
    Note that this data structure is also in-memory. Rather than using it to optimize
    block transfers from the disk to the main memory (as in classic database systems),
    Rust's developers use the BTree to optimize the transfers between the
    main memory and the _processor cache_.

    This data structure performs well on the _duration_ part of the query, while for
    pure range queries it is equivalent to a linear scan of the dataset.

    Note that this data structure is *not* a `B+-tree`, so iteration over ranges is not
    super efficient because there are no links between adjacent leaves.

`Grid2D`

:   An adaptive two-dimensional grid, which indexes intervals as if they were
    points in a two-dimensional space.
    To query this data structure, we find all the grid cells that can overlap
    with the query, like in the shaded area of the following figure

    ```
      end times     +.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
                    |.................|
      +------------------------------+ Query, with
                    |                   swapped endpoints.
                    |
                    |
                    +---------------------------------------+
                                                        start times
    ```

    The data structure is adaptive in the sense that rather than fixing the 
    breakpoints between grid cells at fixed intervals, we use the empirical cumulative
    distribution functions of both the start and end boundaries of the intervals to
    put the same amount of points in all the cells.

    This data structure is expected to behave well on range queries, with
    duration queries being equivalent to linear scans.

    The data structure is parameterized by the number of cells along each dimension.

`Grid3D`

:   Just like its two-dimensional counterpart, but adds a third dimension for
    the duration, also using an adaptive approach. Is expected to work well for
    range-duration queries.

    I couldn't find references for this in the literature. Therefore, while simple, 
    it might be novel.

    The data structure is parameterized by the number of cells along each dimension.

`PeriodIndex`

:   The data structure proposed in the paper. This is the static variant, I have
    yet to tackle the adaptive one.
    Should perform well on both range and duration queries.

    My concern with this data structure is that each interval is potentially stored multiple times. This is a unicum among the considered data structures, and requires
    removing the duplicates. While this operation can be done in constant time, it
    does not come for free.

    The data structure is parameterized by the number of buckets and the maximum number of levels within each bucket.

`NestedBTree` and `NestedVecs`

:   Both data structures are based on the same idea: intervals are partitioned by duration, with indexing in
    place to be able to access the group of intervals with a given duration efficiently. Within each group,
    intervals are indexed by their start time.^[End times can be omitted, since intervals in the same group
    all share the same length.]
    Both indices should allow for efficient range queries.

    Querying works as follows. For a given range of durations to be queried, we select all the groups whose 
    duration falls within the query range. Then, let $(s, e)$ be the time range use for the overlap part
    of the query. On group associated with the duration $d$, we get all the start points $x$ such that
    \[
      s - d < x < e
    \]

    The two implementations use different means of answering the range queries. `NestedBTree` uses a `BTree`
    to map duration to each subgroup. Each subgroup is then indexed with another `BTree on the starting`
    times.
    `NestedVecs` instead keeps all the starting times of intervals of the same duration in a sorted array.
    These arrays are then arranged themselves in an array sorted by the duration of the group.
    Queries are then answered by means of binary search.
    This second implementation has the advantage of begin very cache efficient in the iteration within the
    returned range, but has the drawback of not being implementable in an actual database.
    We use it to see how fast this approach can potentially go.


## Range-duration queries

We consider a dataset of $n=10\,000\,000$ intervals, with starting times uniformly distributed between
0 and $n$.
Durations are distributed according to a zipf law with exponent 1.
^[The experiments have beed run on the host `ironmaiden`]

The aim of this experiment is to test a workload where the overlap of intervals
plays some role, unlike the workload used in the conference version of the paper (see the appendix).

The following figure reports the throughput of each algorithm. Parameterized algorithms report
several parameterizations.

```{r fig.fullwidth=TRUE, fig.width=10, fig.cap="Query throughput (in queries per second) of different indices with different parameterization, on a dataset of ten million intervals."}
readd(plot_one_million)
```

The `NestedVecs` data structure is by far the best performing, followed by the three dimensional grid.
`NestedBTree` is quite a lot slower than its vector-based sibiling: using a linked structure apparently
leaves quite a lot of performance on the table.
All the other algorithms are in a completely different league, performing pretty similarly to one another
(at least in terms of order of magnitude) and being one to two orders of magnitude slower than the leading
group.

In the next figure we consider the distribution of query times of the 5000 queries (left), for the best
configuration of each algorithm. The algorithms are sorted by increasing median query time, which is reported
below the little triangle. The number shown beside the small arrow is the $90$-th percentile query time.

To take into account the fact that a query with many results requires some time just to iterate through the 
output, we also consider the _normalized query time_, which is an _output sensitive_ measure: we divide
the query time by the size of the output of the given query (right plot).

```{r fig.fullwidth=TRUE, fig.width=10, fig.height=6, fig.dpi=300}
plot_grid(
  readd(plot_latency_both),
  readd(plot_normalized_latency_both)
)
```

Looking at both metrics the ranking between the algorithms does not really change.
The interesting thing is that the ranking changes between this pair of figures and the previous
one.
Now `PeriodIndex` ranks second, with a median query time pretty close to the very fast
`NestedVecs` data structure, whereas in terms of throughput it falls very far behind.
This difference is explained by looking at the 90-th percentile of the query time, which shows how 
`PeriodIndex` has and _extremely_ long tail: $10\%$ of the intervals have a query time that is
two orders of magnitude slower than the median!


## Range only queries 

This is the same workload as before, with the duration part of the query omitted.
My expectation here is for data structures that explicitly support range queries (all except linear scan and `BTree`) to perform well.

```{r fig.fullwidth=TRUE, fig.widht=10}
readd(plot_range_only)
```

The plot above meets the expectations, but the throughput is lower.
The reason is explained by distribution plots below: 

```{r fig.fullwidth=TRUE, fig.width=10, fig.height=6, fig.dpi=300}
plot_grid(
  readd(plot_latency_range),
  readd(plot_normalized_latency_range)
)
```

Looking at the normalized query time we see that it is actually slower than with the range-duration 
workload: the reduced throughput is thus just a consequence of the larger size of the output.

We also observe a phenomenon similar to the one observed in the previous section: the interval tree is
the fastest to answer queries but its distribution has a very long tail, which hurts the throughput.

What surprises me is the performance of `NestedVecs`, which I expected to be better. Apparently 
doing several binary searches is detrimental for performance.

## Duration only queries 

Again, we have a similar workload here, but just with the duration part. The expectation now is
to have `BTree` to perform really well.

```{r fig.fullwidth=TRUE, fig.widht=10}
readd(plot_duration_only)
```

The performance of `NestedVecs` is pretty similar.
Curiously, the period index does not support well this workload.

As for the distribution of query times we have that the top ranked algorithms have a pretty
concentrated distribution.

```{r fig.fullwidth=TRUE, fig.width=10, fig.height=6, fig.dpi=300}
plot_grid(
  readd(plot_latency_duration),
  readd(plot_normalized_latency_duration)
)
```


# Appendix
## {-} Replicating the conference workload [2020-08-04]

First of all, let's try to replicate the results of the 
conference version of the paper [@DBLP:conf/ssd/BehrendDGSVRK19].
In the paper, two datasets are described:

> Two data sets were generated: one with a uniform distribution and one
> where the interval duration follows a ZIPF distribution. The time frame for
> the intervals is fixed from timestamp 0 to 1000. The datasets are created by
> randomly drawing a starting point within this interval. The duration values
> are randomly drawn from [1, 100]. This random function either has the same
> probability for any number (uniform) or will prefer lower numbers (ZIPF).

Of these two datasets we consider, in particular, the second one: intervals
with start time between 0 and 1000, with durations distributed according to a ZIPF law
with exponent 1.
As a query workload, we draw intervals from the same distribution, with query durations
ranging up to twice the length of the intervals.
In particular, we consider one million intervals and 5000 queries.^[The experiments have beed run on the host `ironmaiden`]

Before looking at the results, an observation is in order.
With one million intervals with starting times distributed uniformly at random
in the range $[0, 1000]$, we have 1000 intervals starting at each time point, in
expectation.
This means that a very significant fraction of the intervals overlap with the queries,
making the overlap part of the query almost irrelevant: all the selectivity of the query
is in the duration part.

Therefore, in this experiment I would expect indices which are strong in
filtering durations to be very effective.
The figure below reports the throughput (higher is better) and confirms this expectation.
The `y` axis is labelled with the index name and its parameterization, if any.

```{r fig.fullwidth=TRUE, fig.width=10, fig.cap="Query throughput (in queries per second) of different indices with different parameterization, on a dataset of one million intervals. The workload is similar to the one used in the conference version of the paper."}
readd(plot_conference)
```

The best performing index is the three dimensional grid, with the `BTree` following.

One interesting feature of the above experiment is that data structures are grouped
in blocks, irrespective of the parameterization. In order of decreasing performance we have the `BTree`, the three dimensional grid, the period index, the two dimensional grid, the interval tree, and the linear scan.
This ranking, however, is in my opining severely influenced by the workload choice, and is too much in favor of indices that handle well the duration part of the query, which is the only one exhibiting selectivity in this case.

Compared to the conference version, we have that the period index performs rather poorly. This might be due to some missing optimization in my code.
